{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acce9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09bb76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_backend(X, Y):\n",
    "    \"\"\"\n",
    "    Returns pairwise KL divergence (over all pairs of samples) of two matrices X and Y.\n",
    "    \n",
    "    Takes advantage of POT backend to speed up computation.\n",
    "    \n",
    "    Args:\n",
    "        X: np array with dim (n_samples by n_features)\n",
    "        Y: np array with dim (m_samples by n_features)\n",
    "    \n",
    "    Returns:\n",
    "        D: np array with dim (n_samples by m_samples). Pairwise KL divergence matrix.\n",
    "    \"\"\"\n",
    "    assert X.shape[1] == Y.shape[1], \"X and Y do not have the same number of features.\"\n",
    "\n",
    "    nx = ot.backend.get_backend(X,Y)\n",
    "    \n",
    "    X = X/nx.sum(X,axis=1, keepdims=True)\n",
    "    Y = Y/nx.sum(Y,axis=1, keepdims=True)\n",
    "    log_X = nx.log(X)\n",
    "    log_Y = nx.log(Y)\n",
    "    X_log_X = nx.einsum('ij,ij->i',X,log_X)\n",
    "    X_log_X = nx.reshape(X_log_X,(1,X_log_X.shape[0]))\n",
    "    D = X_log_X.T - nx.dot(X,log_Y.T)\n",
    "    return nx.to_numpy(D)\n",
    "\n",
    "def f(G):\n",
    "    return ot.gromov.gwloss(constC, hC1, hC2, G)\n",
    "\n",
    "def df(G):\n",
    "    return ot.gromov.gwggrad(constC, hC1, hC2, G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f405d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity='euc'\n",
    "norm=False\n",
    "alpha=0.8\n",
    "loss_fun='square_loss'\n",
    "backend=ot.backend.NumpyBackend()\n",
    "nx=backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac0ab0",
   "metadata": {},
   "source": [
    "simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f426f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"/data02/tguo/space_batch_effect/simulate/gtt_output/coordinate_file/\"\n",
    "batch_sim=\"_1\"\n",
    "types=\"_types4\"\n",
    "clusters=pd.read_csv(dirs+\"gtt_clusters\"+batch_sim+types+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "ub=np.unique(clusters['batch'])\n",
    "for i in np.arange(1,len(ub)):\n",
    "    uc=np.intersect1d(clusters['clusters'][clusters['batch']==ub[0]],\n",
    "                     clusters['clusters'][clusters['batch']==ub[i]])\n",
    "    for clust in uc:\n",
    "        embed1=pd.read_csv(dirs+\"embed_\"+ub[0]+\"_\"+str(clust)+batch_sim+types+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        embed2=pd.read_csv(dirs+\"embed_\"+ub[i]+\"_\"+str(clust)+batch_sim+types+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=pd.read_csv(dirs+\"coord_\"+ub[0]+\"_\"+str(clust)+batch_sim+types+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord2=pd.read_csv(dirs+\"coord_\"+ub[i]+\"_\"+str(clust)+batch_sim+types+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=coord1.loc[embed1.index,:]\n",
    "        coord2=coord2.loc[embed2.index,:]\n",
    "        ###每个batch内部spot的空间距离####\n",
    "        a=np.float64(nx.from_numpy(coord1.values[:,:2]))\n",
    "        b=np.float64(nx.from_numpy(coord2.values[:,:2]))\n",
    "        D1=ot.dist(a,a, metric='euclidean')\n",
    "        D2=ot.dist(b,b, metric='euclidean')\n",
    "        if norm:\n",
    "            D1 /= nx.min(D1[D1>0])\n",
    "            D2 /= nx.min(D2[D2>0])\n",
    "        ####两个batch spot的低维表示的距离#####\n",
    "        X1,X2 = nx.from_numpy(embed1.values), nx.from_numpy(embed2.values)\n",
    "        if dissimilarity.lower()=='euclidean' or dissimilarity.lower()=='euc':\n",
    "            M = ot.dist(X1,X2)\n",
    "        else:\n",
    "            s1 = X1 + 0.01\n",
    "            s2 = X2 + 0.01\n",
    "            M = kl_divergence_backend(s1, s2)\n",
    "            M = nx.from_numpy(M)\n",
    "        ####每个batch的spot的分布#####\n",
    "        d1 = nx.ones((embed1.shape[0],))/embed1.shape[0]\n",
    "        d2 = nx.ones((embed2.shape[0],))/embed2.shape[0]\n",
    "        ####计算mapping#####\n",
    "        constC, hC1, hC2 = ot.gromov.init_matrix(D1, D2, d1, d2, loss_fun)\n",
    "        G0 = d1[:, None] * d2[None, :]\n",
    "        res=ot.gromov.cg(d1, d2, (1 - alpha) * M, alpha, f, df, G0, armijo=False, C1=D1, C2=D2, constC=constC)\n",
    "        pi=pd.DataFrame(res,index=embed1.index,columns=embed2.index)\n",
    "        pi.to_csv(dirs+\"gwd_pi_\"+ub[0]+\"_\"+ub[i]+\"_\"+str(clust)+batch_sim+types+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d523fe",
   "metadata": {},
   "source": [
    "DLPFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "101c2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.8\n",
    "dirs=\"/data02/tguo/space_batch_effect/human_DLPFC_10x/gtt_output/coordinate_file/\"\n",
    "samples=np.array(['151507','151508','151509','151510','151669','151670','151671','151672','151673','151674','151675','151676'])\n",
    "samples=samples[[8,9,10,11]]\n",
    "flags=\"\"\n",
    "for i in samples:\n",
    "    flags=flags+\"_\"+i\n",
    "clusters=pd.read_csv(dirs+\"gtt_clusters\"+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "ub=np.unique(clusters['batch'])\n",
    "for i in np.arange(1,len(ub)):\n",
    "    uc=np.intersect1d(clusters['clusters'][clusters['batch']==ub[0]],\n",
    "                     clusters['clusters'][clusters['batch']==ub[i]])\n",
    "    for clust in uc:\n",
    "        embed1=pd.read_csv(dirs+\"embed_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        embed2=pd.read_csv(dirs+\"embed_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=pd.read_csv(dirs+\"coord_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord2=pd.read_csv(dirs+\"coord_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=coord1.loc[embed1.index,:]\n",
    "        coord2=coord2.loc[embed2.index,:]\n",
    "        ###每个batch内部spot的空间距离####\n",
    "        a=np.float64(nx.from_numpy(coord1.values[:,:2]))\n",
    "        b=np.float64(nx.from_numpy(coord2.values[:,:2]))\n",
    "        D1=ot.dist(a,a, metric='euclidean')\n",
    "        D2=ot.dist(b,b, metric='euclidean')\n",
    "        if norm:\n",
    "            D1 /= nx.min(D1[D1>0])\n",
    "            D2 /= nx.min(D2[D2>0])\n",
    "        ####两个batch spot的低维表示的距离#####\n",
    "        X1,X2 = nx.from_numpy(embed1.values), nx.from_numpy(embed2.values)\n",
    "        if dissimilarity.lower()=='euclidean' or dissimilarity.lower()=='euc':\n",
    "            M = ot.dist(X1,X2)\n",
    "        else:\n",
    "            s1 = X1 + 0.01\n",
    "            s2 = X2 + 0.01\n",
    "            M = kl_divergence_backend(s1, s2)\n",
    "            M = nx.from_numpy(M)\n",
    "        ####每个batch的spot的分布#####\n",
    "        d1 = nx.ones((embed1.shape[0],))/embed1.shape[0]\n",
    "        d2 = nx.ones((embed2.shape[0],))/embed2.shape[0]\n",
    "        ####计算mapping#####\n",
    "        constC, hC1, hC2 = ot.gromov.init_matrix(D1, D2, d1, d2, loss_fun)\n",
    "        G0 = d1[:, None] * d2[None, :]\n",
    "        res=ot.gromov.cg(d1, d2, (1 - alpha) * M, alpha, f, df, G0, armijo=False, C1=D1, C2=D2, constC=constC)\n",
    "        pi=pd.DataFrame(res,index=embed1.index,columns=embed2.index)\n",
    "        pi.to_csv(dirs+\"gwd_pi_\"+str(ub[0])+\"_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797a0dd",
   "metadata": {},
   "source": [
    "mouse brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d739f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"/data02/tguo/space_batch_effect/mouse_brain/gtt_output/coordinate_file/\"\n",
    "samples=['anterior1','anterior2']\n",
    "flags=\"\"\n",
    "for i in samples:\n",
    "    flags=flags+\"_\"+i\n",
    "clusters=pd.read_csv(dirs+\"gtt_clusters\"+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "ub=np.unique(clusters['batch'])\n",
    "for i in np.arange(1,len(ub)):\n",
    "    uc=np.intersect1d(clusters['clusters'][clusters['batch']==ub[0]],\n",
    "                     clusters['clusters'][clusters['batch']==ub[i]])\n",
    "    for clust in uc:\n",
    "        embed1=pd.read_csv(dirs+\"embed_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        embed2=pd.read_csv(dirs+\"embed_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=pd.read_csv(dirs+\"coord_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord2=pd.read_csv(dirs+\"coord_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=coord1.loc[embed1.index,:]\n",
    "        coord2=coord2.loc[embed2.index,:]\n",
    "        ###每个batch内部spot的空间距离####\n",
    "        a=np.float64(nx.from_numpy(coord1.values[:,:2]))\n",
    "        b=np.float64(nx.from_numpy(coord2.values[:,:2]))\n",
    "        D1=ot.dist(a,a, metric='euclidean')\n",
    "        D2=ot.dist(b,b, metric='euclidean')\n",
    "        if norm:\n",
    "            D1 /= nx.min(D1[D1>0])\n",
    "            D2 /= nx.min(D2[D2>0])\n",
    "        ####两个batch spot的低维表示的距离#####\n",
    "        X1,X2 = nx.from_numpy(embed1.values), nx.from_numpy(embed2.values)\n",
    "        if dissimilarity.lower()=='euclidean' or dissimilarity.lower()=='euc':\n",
    "            M = ot.dist(X1,X2)\n",
    "        else:\n",
    "            s1 = X1 + 0.01\n",
    "            s2 = X2 + 0.01\n",
    "            M = kl_divergence_backend(s1, s2)\n",
    "            M = nx.from_numpy(M)\n",
    "        ####每个batch的spot的分布#####\n",
    "        d1 = nx.ones((embed1.shape[0],))/embed1.shape[0]\n",
    "        d2 = nx.ones((embed2.shape[0],))/embed2.shape[0]\n",
    "        ####计算mapping#####\n",
    "        constC, hC1, hC2 = ot.gromov.init_matrix(D1, D2, d1, d2, loss_fun)\n",
    "        G0 = d1[:, None] * d2[None, :]\n",
    "        res=ot.gromov.cg(d1, d2, (1 - alpha) * M, alpha, f, df, G0, armijo=False, C1=D1, C2=D2, constC=constC)\n",
    "        pi=pd.DataFrame(res,index=embed1.index,columns=embed2.index)\n",
    "        pi.to_csv(dirs+\"gwd_pi_\"+str(ub[0])+\"_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887208e",
   "metadata": {},
   "source": [
    "10x 冠状面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f09ea8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.8\n",
    "dirs=\"/data02/tguo/space_batch_effect/Hippo/gtt_output/coordinate_file/\"\n",
    "samples=['10X_Normal','10X_DAPI','10X_FFPE']\n",
    "flags=\"\"\n",
    "for i in samples:\n",
    "    flags=flags+\"_\"+i\n",
    "clusters=pd.read_csv(dirs+\"gtt_clusters\"+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "ub=np.unique(clusters['batch'])\n",
    "ub=['10X_Normal','10X_DAPI','10X_FFPE']\n",
    "for i in np.arange(1,len(ub)):\n",
    "    uc=np.intersect1d(clusters['clusters'][clusters['batch']==ub[0]],\n",
    "                     clusters['clusters'][clusters['batch']==ub[i]])\n",
    "    for clust in uc:\n",
    "        embed1=pd.read_csv(dirs+\"embed_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        embed2=pd.read_csv(dirs+\"embed_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=pd.read_csv(dirs+\"coord_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord2=pd.read_csv(dirs+\"coord_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=coord1.loc[embed1.index,:]\n",
    "        coord2=coord2.loc[embed2.index,:]\n",
    "        ###每个batch内部spot的空间距离####\n",
    "        a=np.float64(nx.from_numpy(coord1.values[:,:2]))\n",
    "        b=np.float64(nx.from_numpy(coord2.values[:,:2]))\n",
    "        D1=ot.dist(a,a, metric='euclidean')\n",
    "        D2=ot.dist(b,b, metric='euclidean')\n",
    "        if norm:\n",
    "            D1 /= nx.min(D1[D1>0])\n",
    "            D2 /= nx.min(D2[D2>0])\n",
    "        ####两个batch spot的低维表示的距离#####\n",
    "        X1,X2 = nx.from_numpy(embed1.values), nx.from_numpy(embed2.values)\n",
    "        if dissimilarity.lower()=='euclidean' or dissimilarity.lower()=='euc':\n",
    "            M = ot.dist(X1,X2)\n",
    "        else:\n",
    "            s1 = X1 + 0.01\n",
    "            s2 = X2 + 0.01\n",
    "            M = kl_divergence_backend(s1, s2)\n",
    "            M = nx.from_numpy(M)\n",
    "        ####每个batch的spot的分布#####\n",
    "        d1 = nx.ones((embed1.shape[0],))/embed1.shape[0]\n",
    "        d2 = nx.ones((embed2.shape[0],))/embed2.shape[0]\n",
    "        ####计算mapping#####\n",
    "        constC, hC1, hC2 = ot.gromov.init_matrix(D1, D2, d1, d2, loss_fun)\n",
    "        G0 = d1[:, None] * d2[None, :]\n",
    "        res=ot.gromov.cg(d1, d2, (1 - alpha) * M, alpha, f, df, G0, armijo=False, C1=D1, C2=D2, constC=constC)\n",
    "        pi=pd.DataFrame(res,index=embed1.index,columns=embed2.index)\n",
    "        pi.to_csv(dirs+\"gwd_pi_\"+str(ub[0])+\"_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ce578",
   "metadata": {},
   "source": [
    "mouse ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ba8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "dirs=\"/data02/tguo/space_batch_effect/mouse_OB/gtt_output/coordinate_file/\"\n",
    "samples=['BGI','SlideV2','10X']\n",
    "# samples=['BGI','SlideV2','scRNA']\n",
    "flags=\"\"\n",
    "for i in samples:\n",
    "    flags=flags+\"_\"+i\n",
    "clusters=pd.read_csv(dirs+\"gtt_clusters\"+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "ub=np.unique(clusters['batch'])\n",
    "ub=samples\n",
    "for i in np.arange(1,len(ub)):\n",
    "    uc=np.intersect1d(clusters['clusters'][clusters['batch']==ub[0]],\n",
    "                     clusters['clusters'][clusters['batch']==ub[i]])\n",
    "    for clust in uc:\n",
    "        embed1=pd.read_csv(dirs+\"embed_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        embed2=pd.read_csv(dirs+\"embed_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=pd.read_csv(dirs+\"coord_\"+str(ub[0])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord2=pd.read_csv(dirs+\"coord_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\",header=0,index_col=0,sep=\",\")\n",
    "        coord1=coord1.loc[embed1.index,:]\n",
    "        coord2=coord2.loc[embed2.index,:]\n",
    "        ###每个batch内部spot的空间距离####\n",
    "#         a=np.float64(nx.from_numpy(coord1.values[:,:2]))\n",
    "#         b=np.float64(nx.from_numpy(coord2.values[:,:2]))\n",
    "        a=np.float64(nx.from_numpy(coord1.values))\n",
    "        b=np.float64(nx.from_numpy(coord2.values))\n",
    "        D1=ot.dist(a,a, metric='euclidean')\n",
    "        D2=ot.dist(b,b, metric='euclidean')\n",
    "        if norm:\n",
    "            D1 /= nx.min(D1[D1>0])\n",
    "            D2 /= nx.min(D2[D2>0])\n",
    "        ####两个batch spot的低维表示的距离#####\n",
    "        X1,X2 = nx.from_numpy(embed1.values), nx.from_numpy(embed2.values)\n",
    "        if dissimilarity.lower()=='euclidean' or dissimilarity.lower()=='euc':\n",
    "            M = ot.dist(X1,X2)\n",
    "        else:\n",
    "            s1 = X1 + 0.01\n",
    "            s2 = X2 + 0.01\n",
    "            M = kl_divergence_backend(s1, s2)\n",
    "            M = nx.from_numpy(M)\n",
    "        ####每个batch的spot的分布#####\n",
    "        d1 = nx.ones((embed1.shape[0],))/embed1.shape[0]\n",
    "        d2 = nx.ones((embed2.shape[0],))/embed2.shape[0]\n",
    "        ####计算mapping#####\n",
    "        constC, hC1, hC2 = ot.gromov.init_matrix(D1, D2, d1, d2, loss_fun)\n",
    "        G0 = d1[:, None] * d2[None, :]\n",
    "        res=ot.gromov.cg(d1, d2, (1 - alpha) * M, alpha, f, df, G0, armijo=False, C1=D1, C2=D2, constC=constC,numItermax=100000,numItermaxEmd=1e6)\n",
    "        pi=pd.DataFrame(res,index=embed1.index,columns=embed2.index)\n",
    "        pi.to_csv(dirs+\"gwd_pi_\"+str(ub[0])+\"_\"+str(ub[i])+\"_\"+str(clust)+flags+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee736f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
